# NGRS Solver - AI Agent Guidelines

## Project Overview
**NGRS Solver v0.95** is a constraint programming-based shift scheduling optimizer using Google OR-Tools CP-SAT. It exposes a FastAPI REST API with async job processing via Redis, serving security roster scheduling for Singapore operations.

**Core Stack**: Python 3.12+, OR-Tools 9.11, FastAPI 0.115, Redis 7.0, Docker  
**Production**: `https://ngrssolver09.comcentricapps.com` (Ubuntu 22.04 EC2)

---

## Architecture: Hybrid Constraint Model

### Three-Layer Design
1. **API Layer** (`src/api_server.py`) - FastAPI endpoints with Redis-backed async workers
2. **Solver Engine** (`context/engine/solver_engine.py`) - CP-SAT model builder and constraint loader
3. **Constraint Modules** (`context/constraints/*.py`) - 31 modular hard/soft constraints (C1-C17, S1-S16)

### Critical Distinction: Input-Driven Parameters vs. Hard-Coded Logic
- **Input JSON** provides `constraintList` and `solverScoreConfig` to **configure existing constraints**
- **Constraint modules** (`C1_mom_daily_hours.py`, etc.) contain the actual CP-SAT logic
- The solver **does not dynamically generate constraints** from JSON—it reads parameters that modify pre-built modules
- See [implementation_docs/CONSTRAINT_ARCHITECTURE.md](implementation_docs/CONSTRAINT_ARCHITECTURE.md) for data flow

### Key Components
```
context/engine/solver_engine.py     # CP-SAT model builder, dynamic constraint loading
context/engine/data_loader.py       # JSON → ctx dict parser
context/constraints/*.py            # Individual constraint implementations
src/api_server.py                   # FastAPI app with 3 main endpoints
src/redis_worker.py                 # Background async job processor
src/redis_job_manager.py            # Redis-backed job queue/storage
src/output_builder.py               # Result formatter with hour breakdowns
```

---

## Critical Workflows

### Running Solvers Locally
```bash
# CLI solver (no API)
python src/run_solver.py --in input/my_input.json --time 300

# FastAPI server (sync + async endpoints)
uvicorn src.api_server:app --reload --port 8080

# Docker Compose (API + Redis)
docker-compose up
```

### Testing
```bash
# Run all tests
pytest -q

# Specific constraint tests
pytest tests/test_week_ot_caps.py -v

# Context pack smoke test
pytest context/tests/test_constraints_smoke.py
```

### Deployment (EC2 Production)
```bash
# From local machine
scp -r . ubuntu@<ec2-ip>:~/ngrs-solver
ssh ubuntu@<ec2-ip>

# On EC2
cd ~/ngrs-solver
./deploy/ec2-setup.sh          # First-time setup (Docker, Redis)
sudo systemctl restart ngrs    # Deploy code changes
```
- Uses systemd service (`/etc/systemd/system/ngrs.service`)
- Redis runs in Docker container
- API runs via uvicorn with 2 workers
- See [deploy/README.md](deploy/README.md) for full guide

---

## Code Conventions

### Constraint Implementation Pattern
All constraint modules follow this structure (see `context/constraints/C1_mom_daily_hours.py`):
```python
def apply(model, ctx, slots, assignments):
    """Add CP-SAT hard constraints to model during build phase."""
    # Add constraints using model.Add(...)
    pass

def score_violations(ctx, assignments, score_book):
    """Score violations post-solve (soft constraints only)."""
    # Calculate penalties and call score_book.add_violation(...)
    pass
```
- **Hard constraints** (C1-C17) implement `apply()` only—violations are impossible
- **Soft constraints** (S1-S16) implement `score_violations()`—violations are penalized
- Constraints auto-load via `pkgutil` in `solver_engine.py`

### Data Model: The `ctx` Dictionary
The `ctx` dict is the central data structure passed throughout the solver:
```python
ctx = {
    'employees': [...],           # List of employee dicts
    'demandItems': [...],         # Shift requirements
    'slots': [...],               # Generated Slot objects
    'x': {...},                   # CP-SAT decision variables x[(slot_id, emp_id)]
    'model': CpModel(),           # OR-Tools model
    'constraintList': [...],      # Input constraint configs
    'solverScoreConfig': {...},   # Scoring weights
    'planningReference': {...},   # Metadata
    'workRequirements': [...],    # ICPMP v3 requirements (optional)
}
```
- Built by `data_loader.load_input()`
- Slots generated by `slot_builder.build_slots()`
- Decision variables `x` created in `solver_engine.build_model()`

### Schema Versions
- **Input**: v0.70 or v0.95 (check `schemaVersion` field)
- **Output**: v0.95 (standardized)
- **ICPMP Tool**: v3.0 (configuration optimizer, separate endpoint `/configure`)

---

## Per-Requirement Auto-Optimization (v0.95)

### Ratio Caching System (91% Time Savings)
The solver tests multiple `strictAdherenceRatio` values (60%-80%) to minimize employee count, then caches results:
```bash
# View cache
python src/manage_ratio_cache.py stats

# Clear cache
python src/manage_ratio_cache.py clear
```
- Cache file: `config/ratio_cache.json`
- Pattern-based keys (work pattern + employee count hash)
- See [docs/RATIO_CACHING_GUIDE.md](docs/RATIO_CACHING_GUIDE.md)

### Configuration Flow
```json
{
  "workRequirements": [
    {
      "requirementId": "R1",
      "autoOptimize": true,           // ← Enable per-requirement
      "minStrictRatio": 0.6,
      "maxStrictRatio": 0.8,
      "strictRatioStep": 0.1
    }
  ]
}
```
- Optimization runs in `redis_worker.py` before solve
- Implemented in `context/engine/config_optimizer_v3.py`

---

## API Endpoints (FastAPI)

**IMPORTANT - Testing Protocol**:
- **ALWAYS use production endpoint** for testing: `https://ngrssolver09.comcentricapps.com`
- **Default to /solve/async** for all test requests (supports long-running jobs)
- **Use /solve/async/{job_id}/result** to obtain final results
- Never use local endpoints unless explicitly requested by the user

### 1. POST /solve (Synchronous)
Immediate response, blocks until solve completes (use for small problems <10s):
```bash
curl -X POST http://localhost:8080/solve \
  -H "Content-Type: application/json" \
  -d @input_1211_optimized.json
```

### 2. POST /solve/async (Asynchronous + Webhooks) **[PREFERRED FOR TESTING]**
Submit job → get job_id → poll status OR receive webhook:
```bash
# Submit job to PRODUCTION (always use this for testing)
curl -X POST https://ngrssolver09.comcentricapps.com/solve/async \
  -H "Content-Type: application/json" \
  -d @"/path/to/input.json"

# Check job status
curl https://ngrssolver09.comcentricapps.com/solve/async/{job_id}

# Get final results (use this endpoint, not just /solve/async/{job_id})
curl https://ngrssolver09.comcentricapps.com/solve/async/{job_id}/result

# Optional: Submit with webhook
curl -X POST https://ngrssolver09.comcentricapps.com/solve/async \
  -H "Content-Type: application/json" \
  -d '{"input": {...}, "webhookUrl": "https://webhook.site/xyz"}'
```
- Jobs persist in Redis (`RedisJobManager`)
- Workers process via `redis_worker.py`
- TTL: 1 hour (configurable)
- **Note**: Production uses Ubuntu's built-in Redis server (no Docker container needed)

### 3. POST /configure (ICPMP v3)
Configuration optimizer—suggests work patterns, staffing levels, rotation offsets:
```bash
curl -X POST http://localhost:8080/configure \
  -H "Content-Type: application/json" \
  -d @test_api_icpmp.json
```
- Different input schema from `/solve`
- Implemented in `context/engine/config_optimizer_v3.py`
- Returns recommended patterns + employee counts

---

## Common Pitfalls

### 1. Rotation Offset Management
**Problem**: Offsets must be staggered (0, 1, 2, ...) for rotation patterns to work correctly.  
**Solution**: Always call `ensure_staggered_offsets()` before solver runs:
```python
from src.offset_manager import ensure_staggered_offsets
input_data = ensure_staggered_offsets(input_data)
```
- Used in `redis_worker.py` and CLI solver
- See [docs/OFFSET_MANAGEMENT.md](docs/OFFSET_MANAGEMENT.md)

### 2. CP-SAT Parallelization
**Problem**: Large problems need multi-threading for speed.  
**Solution**: Set `num_search_workers` based on problem size:
```python
num_workers = calculate_num_search_workers(num_slots, num_employees)
solver.parameters.num_search_workers = num_workers
```
- Adaptive: 1-16 workers based on complexity
- Override with `CPSAT_NUM_THREADS` env var
- Implemented in `solver_engine.py`

### 3. Hour Breakdown Calculations
**Problem**: MOM compliance requires accurate normal/OT/public-holiday hour splits.  
**Solution**: Use `calculate_mom_compliant_hours()` from `time_utils.py`:
```python
from context.engine.time_utils import calculate_mom_compliant_hours

hours = calculate_mom_compliant_hours(
    start_dt=assignment['startDateTime'],
    end_dt=assignment['endDateTime'],
    employee_id=emp_id,
    assignment_date_obj=date_obj,
    all_assignments=assignments
)
# Returns: {normalHours, overtimeHours, publicHolidayHours, restDayPay}
```
- Used in `output_builder.py` for all assignments
- Handles cross-midnight shifts, PH detection, weekly/monthly caps

### 4. Redis Connection Issues
**Problem**: Workers fail if Redis unavailable.  
**Solution**: Check Redis health before starting workers:
```python
from src.redis_manager import get_redis_client
redis_client = get_redis_client()  # Raises exception if connection fails
```
- Redis URL: `REDIS_URL` env var (default: `redis://localhost:6379`)
- Docker Compose automatically starts Redis

---

## Documentation Structure

### Quick References
- **[implementation_docs/FASTAPI_QUICK_REFERENCE.md](implementation_docs/FASTAPI_QUICK_REFERENCE.md)** - Essential API commands (2 min)
- **[implementation_docs/DOCUMENTATION_INDEX.md](implementation_docs/DOCUMENTATION_INDEX.md)** - Master doc index

### Technical Deep Dives
- **[implementation_docs/CONSTRAINT_ARCHITECTURE.md](implementation_docs/CONSTRAINT_ARCHITECTURE.md)** - How constraints work
- **[docs/RATIO_CACHING_GUIDE.md](docs/RATIO_CACHING_GUIDE.md)** - Auto-optimization details
- **[implementation_docs/CPSAT_UNDERSTANDING.md](implementation_docs/CPSAT_UNDERSTANDING.md)** - CP-SAT solver patterns

### Domain Knowledge
- **[context/glossary.md](context/glossary.md)** - Terms (Demand Item, PDL, Delta Solve, etc.)
- **[context/domain/planning_objects.md](context/domain/planning_objects.md)** - Data model entities
- **[context/domain/rotation_patterns.md](context/domain/rotation_patterns.md)** - Work pattern definitions

---

## When Making Changes

### Adding/Modifying Constraints
1. Identify constraint file: `context/constraints/C*_*.py` or `S*_*.py`
2. Understand hard vs. soft: Hard = `apply()`, Soft = `score_violations()`
3. Test with minimal input: See `input/` folder for examples
4. Update `solverScoreConfig` weights if needed
5. Run `pytest context/tests/test_constraints_smoke.py`

### Modifying API Endpoints
1. Edit `src/api_server.py`
2. Update Pydantic models in `src/models.py` if schema changes
3. Test with curl or Postman (collection in `postman/`)
4. Update [implementation_docs/FASTAPI_QUICK_REFERENCE.md](implementation_docs/FASTAPI_QUICK_REFERENCE.md)

### Changing Output Format
1. Modify `src/output_builder.py` (async) or `src/run_solver.py` (CLI)
2. Ensure `schemaVersion` bumped if breaking change
3. Test with existing frontend integrations
4. Document in [README.md](README.md)

---

## Environment Variables

```bash
# API Configuration
PORT=8080
CORS_ORIGINS=http://localhost:3000
ADMIN_API_KEY=change-me-in-production

# Redis Configuration
REDIS_URL=redis://localhost:6379
REDIS_KEY_PREFIX=ngrs

# Worker Configuration
NUM_ASYNC_WORKERS=2          # Background job processors
RESULT_TTL_SECONDS=3600      # Job result expiration

# Solver Configuration
CPSAT_NUM_THREADS=4          # Override adaptive parallelization
MAX_SOLVE_TIME_SECONDS=300   # Global timeout
```

---

## Testing Strategy

### Unit Tests
- Constraint logic: `tests/test_week_ot_caps.py`
- Hour calculations: `tests/test_mom_hours.py`
- Context pack: `context/tests/test_constraints_smoke.py`

### Integration Tests
- ICPMP v3: `test_icpmp_v3.py`
- API endpoints: `test_icpmp_integration.py`
- Full solver: `quick_solve.py` (manual)

### Debugging Scripts
```bash
# Check input validity
python verify_input.py --in input/my_input.json

# Analyze preprocessing
python check_preprocessing.py --in input/my_input.json

# Debug constraint filtering
python debug_filtering.py --in input/my_input.json
```

---

## Key Files Reference

| File | Purpose |
|------|---------|
| `src/api_server.py` | FastAPI app, endpoint definitions |
| `src/redis_worker.py` | Background job processor |
| `context/engine/solver_engine.py` | CP-SAT model builder, constraint loader |
| `context/engine/data_loader.py` | Input JSON → ctx dict parser |
| `context/engine/time_utils.py` | MOM hour calculations |
| `src/output_builder.py` | Result formatting for async API |
| `src/run_solver.py` | CLI solver (no API) |
| `src/configure_roster.py` | ICPMP tool CLI wrapper |
| `src/offset_manager.py` | Rotation offset staggering |
| `src/ratio_cache.py` | Auto-optimization cache |
| `src/feasibility_checker.py` | Pre-solve validation |

---

## Success Metrics

- **Solve Time**: 15-45 min for 200-300 employee rosters
- **Cache Hit Rate**: ~91% time savings on repeated patterns
- **API Latency**: <100ms for `/health`, <2s for feasibility checks
- **Worker Throughput**: 2 concurrent jobs (configurable)
- **Production Uptime**: 99%+ (systemd auto-restart)
